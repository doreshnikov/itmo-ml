{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd().endswith('lab02_linear'):\n",
    "    os.chdir('..')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lab02_linear.common.data import read\n",
    "from lab02_linear.common.graders import prediction, smape\n",
    "\n",
    "fin = open('lab02_linear/resources/LR/5.txt')\n",
    "m = int(fin.readline().strip())\n",
    "n1, xtrain, ytrain = read(fin)\n",
    "n2, xtest, ytest = read(fin)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Сразу хорошие результаты\n",
    "\n",
    "Покажем, что градиентный спуск вполне себе неплох и умеет добиваться неплохих результатов при определенных параметрах"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from lab02_linear.methods.gradient import solve as g_solve\n",
    "\n",
    "alpha, score = next(g_solve(\n",
    "    10000, xtrain, ytrain,\n",
    "    do_normalize=False, fill='zero', lmbd=lambda step: 0.8 / math.pow(step + 1, 0.3)\n",
    "))\n",
    "yptest = prediction(alpha, xtest)\n",
    "print(score, '-', smape(yptest, ytest))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Теперь чуть более упорядоченно\n",
    "\n",
    "### 1. Least squares"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lab02_linear.methods.least_squares import solve as ls_solve\n",
    "\n",
    "for do_normalize in [True, False]:\n",
    "    try:\n",
    "        alpha, score, err = ls_solve(xtrain, ytrain, do_normalize)\n",
    "        yptest = prediction(alpha, xtest)\n",
    "        print(score, '-', smape(yptest, ytest), '/', err)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print('fail')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Добавим небольшую ridge-составляющую для случая плохой обсуловленности:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for do_normalize in [True, False]:\n",
    "    for ridge in np.hstack(([0], np.power(10., range(-20, 1, 3)))):\n",
    "        try:\n",
    "            alpha, score, err = ls_solve(xtrain, ytrain, do_normalize, ridge)\n",
    "            yptest = prediction(alpha, xtest)\n",
    "            print(f'norm: {do_normalize}, ridge: {ridge} ->', '%.9f' % score, '-', '%.9f' % smape(yptest, ytest), '/', err)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(f'norm: {do_normalize}, ridge: {ridge} -> fail')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как видно, у такого подхода достаточно большие проблемы, поэтому стоит попробовать сделать регрессию с помощью SVD-декомпозиции. Переберем все возможные параметры, которые в целом можно настраивать (нормализацию и ридж-составляющую):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lab02_linear.methods.least_squares import solve_svd as svd_solve\n",
    "\n",
    "for do_normalize in [True, False]:\n",
    "    for ridge in np.hstack(([0], np.power(10., range(-8, 1, 2)))):\n",
    "        try:\n",
    "            alpha, score, err = svd_solve(xtrain, ytrain, do_normalize, ridge)\n",
    "            yptest = prediction(alpha, xtest)\n",
    "            print(f'norm: {do_normalize}, ridge: {ridge} ->', '%.9f' % score, '-', '%.9f' % smape(yptest, ytest), '/', err)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(f'norm: {do_normalize}, ridge: {ridge} -> fail')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Заметим, что поведение примерно ожидаемое - при SVD-декомпозиции в ридж-составляющей нет нужды, более того, нет ошибок о невозможности инвертировать матрицу."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "_, _, _ = svd_solve(xtrain, ytrain, True, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Градиентный спуск\n",
    "\n",
    "Здесь можно перебирать коэффициент при градиенте и его затухание, изначальное заполнение, нормализацию и регуляризацию. Поскольку полный перебор будет работать слишком долго, понадеемся на рандом и сделаем ограниченное число шагов. 1000 итераций работает примерно секунду - попробуем сначала запустить с таким ограничением, подберем оптимальные параметры и дальше будем перебирать число итераций."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "steps = 20\n",
    "\n",
    "total_norm = [0, 0]\n",
    "total_fill = [0, 0, 0]\n",
    "total_reg = [0, 0, 0]\n",
    "\n",
    "for t1, do_normalize in enumerate([True, False]):\n",
    "    for t2, fill in enumerate(['zero', 'uniform', 'smart']):\n",
    "        for t3, reg in enumerate([0, 0.0001, 0.01]):\n",
    "            for i in range(steps):\n",
    "                c = np.random.uniform(0.1, 1)\n",
    "                deg = np.random.uniform(-4, 4)\n",
    "                p = 2 * np.random.uniform(0, 1) ** 2\n",
    "                alpha, score = next(g_solve(\n",
    "                    1000, xtrain, ytrain,\n",
    "                    do_normalize, fill, lmbd=lambda step: c * (math.pow(10., deg)) / math.pow(step + 1, p)\n",
    "                ))\n",
    "                yptest = prediction(alpha, xtest)\n",
    "                score = smape(yptest, ytest)\n",
    "                if score < 0.1:\n",
    "                    print(f'{score}: {do_normalize}, {fill}, {reg}, {c}x10^{deg}/t^{p}')\n",
    "                    \n",
    "                total_norm[t1] += score\n",
    "                total_fill[t2] += score\n",
    "                total_reg[t3] += score\n",
    "            print('+1/18')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(total_norm, total_fill, total_reg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Найти что-то хорошее не получилось, но можно попробовать зафиксировать какие-то параметры и подвигать другие. Более того, будем запоминать лучшее из встреченных решений по пути (сначала проверим на известном хорошем решении):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from lab02_linear.methods.gradient import memoized_solve as mg_solve\n",
    "\n",
    "alpha, score = mg_solve(\n",
    "    10000, xtrain, ytrain,\n",
    "    do_normalize=False, fill='zero', lmbd=lambda step: 0.8 / math.pow(step + 1, 0.3)\n",
    ")\n",
    "yptest = prediction(alpha, xtest)\n",
    "print(score, '-', smape(yptest, ytest))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Если честно, то и руками, и какими-то минимальными переборами получить что-то лучше не очень выходит, поэтому можно остановиться на этом варианте и построить для него графики:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_scores, test_scores = [], []\n",
    "for alpha, score in g_solve(10000, xtrain, ytrain, sequential=True):\n",
    "    yptest = prediction(alpha, xtest)\n",
    "    train_scores.append(score)\n",
    "    test_scores.append(smape(yptest, ytest))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(train_scores)\n",
    "plt.plot(test_scores, color='red')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как видно, градиентный спуск действительно сходится в какой-то локальный минимум, но, судя по скору, это не глобальный минимум. Можно было бы как-то подобрать оптимальные параметры, но пока что ни один адекватный поиск до них не дошел."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Отжиг"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lab02_linear.methods.annealing import solve\n",
    "\n",
    "alpha, score = next(solve(100000, xtrain, ytrain))\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_scores_a, test_scores_a = [], []\n",
    "for alpha, score in solve(10000, xtrain, ytrain, sequential=True):\n",
    "    yptest = prediction(alpha, xtest)\n",
    "    train_scores_a.append(score)\n",
    "    test_scores_a.append(smape(yptest, ytest))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(train_scores)\n",
    "plt.plot(test_scores, color='red')\n",
    "plt.plot(train_scores_a, color='green')\n",
    "plt.plot(test_scores_a, color='yellow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Параметры отжига можно настраивать еще дольше, чем параметры градиентного спуска (force_downhill, temperature, mutations), поэтому можно ограничиться текущим графиком, сказав, что он застрял в менее удачном локальном минимуме."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Вывод\n",
    "\n",
    "Наиболее точный метод - метод наименьших квадратов. Его использование сопряжено с тем, что для большинства матриц вычисленное напрямую значение псевдообратной будет некорректно из-за потерь в точности и несогласованности данных. Для этого + для ускорения программы можно использовать разложения (QR, SVD). На достаточно неплохих данных такой метод сразу дает наилучший результат без необходимости проводить какие-либо итерации.\n",
    "\n",
    "Метод градиентного спуска гарантированно (при правильной настройке) сходится в некоторый локальный минимум, однако чтобы он сошелся в глобальный, требуется очень тщательно подбирать параметры и выбирать начальные приближения, что не всегда возможно в реалиях ограниченного времени.\n",
    "\n",
    "Итерационные генетические алгоритмы или тот же отжиг напоминают градиентный спуск тем, что постепенно приближаются к оптимальному значению, однако так же требуют гибкой настройки.\n",
    "Преимущество градиентного спуска в том, что по графику его результатов проще понять, как надо перенастроить параметры, чтобы достичь лучшего результата, тогда как зависимость между параметрами генетического алгоритма и его поведением обычно менее очевидна и связь с задачей менее интерпретируема.\n",
    "В случае отжига можно провести параллель между температурой и затухающим learning_rate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}